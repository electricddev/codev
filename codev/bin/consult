#!/usr/bin/env python3
"""Consult tool - wrapper for gemini-cli, codex CLI, and claude CLI.

Provides a unified interface for single-agent consultation via external AI CLIs.
Each invocation is stateless (fresh process).

Usage:
    # Original consultation mode (default)
    consult gemini "Review this design"
    consult codex "What do you think of this API?"
    consult claude "Review this implementation"
    consult pro "Review this"         # alias for gemini
    consult gpt "Review this"         # alias for codex
    consult opus "Review this"        # alias for claude
    echo "Review this" | consult pro

    # PR review mode (optimized)
    consult pr 33                     # All models in parallel
    consult pr 33 --model gemini      # Single model
    consult pr 33 --all               # Explicit all models
    consult pr 33 --dry-run           # Show what would be fetched

For parallel consultation, run multiple consult commands in separate shells.
"""

import concurrent.futures
import glob as glob_module
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

# Note: This script now uses manual argument parsing instead of Typer
# to support hybrid positional args (consult MODEL QUERY) with subcommands (consult pr N)

# Model aliases (alias -> canonical name)
MODEL_MAP = {
    "pro": "gemini",
    "gpt": "codex",
    "opus": "claude",
}

# Derive canonical model list from MODEL_MAP values
ALL_MODELS = sorted(set(MODEL_MAP.values()))


def load_dotenv(codev_root: Path) -> None:
    """Load .env file from codev root if it exists."""
    env_file = codev_root / ".env"
    if not env_file.exists():
        return

    with open(env_file) as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                key, _, value = line.partition("=")
                key = key.strip()
                value = value.strip()
                # Remove surrounding quotes if present
                if (value.startswith('"') and value.endswith('"')) or \
                   (value.startswith("'") and value.endswith("'")):
                    value = value[1:-1]
                # Only set if not already in environment
                if key not in os.environ:
                    os.environ[key] = value


def find_codev_root() -> Path:
    """Find the codev root directory by walking up from cwd."""
    current = Path.cwd()
    for parent in [current] + list(current.parents):
        role_file = parent / "codev" / "roles" / "consultant.md"
        if role_file.exists():
            return parent
    return current  # Fallback to cwd


def get_role(role_file: Path) -> str:
    """Read the consultant role definition."""
    if not role_file.exists():
        print(f"Error: Role file not found: {role_file}", file=sys.stderr)
        print("Are you in a codev-enabled project?", file=sys.stderr)
        sys.exit(1)
    return role_file.read_text()


def log_query(log_dir: Path, model: str, query: str, duration_secs: float | None = None) -> None:
    """Log consultation to .consult/history.log with optional timing."""
    try:
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / "history.log"
        timestamp = datetime.now().isoformat()
        # Truncate long queries for log readability
        query_preview = query[:100].replace("\n", " ")
        if len(query) > 100:
            query_preview += "..."
        duration_str = f" duration={duration_secs:.1f}s" if duration_secs else ""
        with open(log_file, "a") as f:
            f.write(f"{timestamp} model={model}{duration_str} query={query_preview}\n")
    except Exception:
        # Logging failure should not block consultation
        pass


def run_model_consultation(
    model: str,
    query: str,
    codev_root: Path,
    output_file: Optional[Path] = None,
) -> tuple[str, int, float]:
    """Run a consultation with a specific model.

    Returns: (output, return_code, duration_secs)
    """
    role_file = codev_root / "codev" / "roles" / "consultant.md"
    role = get_role(role_file)

    # Resolve model aliases
    resolved = MODEL_MAP.get(model.lower(), model.lower())
    temp_system_file = None
    env = {}

    if resolved == "gemini":
        if not shutil.which("gemini"):
            return "Error: gemini-cli not found", 1, 0.0
        temp_system_file = tempfile.NamedTemporaryFile(
            mode="w", suffix=".md", delete=False
        )
        temp_system_file.write(role)
        temp_system_file.close()
        cmd = ["gemini", "--yolo", query]
        env = {"GEMINI_SYSTEM_MD": temp_system_file.name}
        if os.environ.get("GOOGLE_API_KEY") and os.environ.get("GEMINI_API_KEY"):
            env["GEMINI_API_KEY"] = ""
    elif resolved == "codex":
        if not shutil.which("codex"):
            return "Error: codex not found", 1, 0.0
        cmd = ["codex", "exec", "--full-auto", query]
        env = {"CODEX_SYSTEM_MESSAGE": role}
    elif resolved == "claude":
        if not shutil.which("claude"):
            return "Error: claude not found", 1, 0.0
        full_query = f"{role}\n\n---\n\nConsultation Request:\n{query}"
        cmd = ["claude", "--print", "-p", full_query, "--dangerously-skip-permissions"]
        env = {}
    else:
        return f"Unknown model: {model}", 1, 0.0

    full_env = {**os.environ, **env}
    start_time = time.time()

    try:
        result = subprocess.run(
            cmd,
            env=full_env,
            capture_output=True,
            text=True,
        )
        duration = time.time() - start_time
        output = result.stdout
        if result.stderr:
            output += f"\n[stderr]: {result.stderr}"

        # Save to output file if specified
        if output_file:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            output_file.write_text(output)

        return output, result.returncode, duration
    except KeyboardInterrupt:
        return "Interrupted", 130, time.time() - start_time
    finally:
        if temp_system_file:
            try:
                os.unlink(temp_system_file.name)
            except Exception:
                pass


def fetch_pr_data(pr_number: int, output_dir: Path) -> dict:
    """Fetch PR data and save to files. Returns metadata."""
    output_dir.mkdir(parents=True, exist_ok=True)
    metadata = {"pr_number": pr_number, "files": []}

    # 1. PR info
    try:
        result = subprocess.run(
            ["gh", "pr", "view", str(pr_number)],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            metadata["pr_info_error"] = result.stderr.strip()
        else:
            (output_dir / "pr-info.txt").write_text(result.stdout)
            metadata["pr_info"] = True

            # Extract spec number from PR title or body
            spec_match = re.search(r'\[Spec (\d+)\]|\bspec[- ]?(\d+)\b', result.stdout, re.IGNORECASE)
            if spec_match:
                metadata["spec_number"] = spec_match.group(1) or spec_match.group(2)
    except Exception as e:
        metadata["pr_info_error"] = str(e)

    # 2. Comments
    try:
        result = subprocess.run(
            ["gh", "pr", "view", str(pr_number), "--comments"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
             metadata["comments_error"] = result.stderr.strip()
        else:
            (output_dir / "pr-comments.txt").write_text(result.stdout)
            metadata["comments"] = True
    except Exception as e:
        metadata["comments_error"] = str(e)

    # 3. Diff
    try:
        result = subprocess.run(
            ["gh", "pr", "diff", str(pr_number)],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
             metadata["diff_error"] = result.stderr.strip()
        else:
            (output_dir / "pr-diff.patch").write_text(result.stdout)
            metadata["diff"] = True
            metadata["diff_lines"] = len(result.stdout.splitlines())
    except Exception as e:
        metadata["diff_error"] = str(e)

    # 4. Files list (JSON)
    try:
        result = subprocess.run(
            ["gh", "pr", "view", str(pr_number), "--json", "files,title,headRefName"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            metadata["files_error"] = result.stderr.strip()
        else:
            files_data = json.loads(result.stdout)
            metadata["files"] = files_data.get("files", [])
            metadata["title"] = files_data.get("title", "")
            metadata["branch"] = files_data.get("headRefName", "")
            (output_dir / "pr-files.json").write_text(result.stdout)
    except Exception as e:
        metadata["files_error"] = str(e)

    # 5. Find spec file (if exists)
    # Note: spec_number from regex is already zero-padded (e.g., "0038" from "[Spec 0038]")
    codev_root = find_codev_root()
    spec_number = metadata.get("spec_number")
    if spec_number:
        spec_pattern = str(codev_root / "codev" / "specs" / f"{spec_number}-*.md")
        spec_files = glob_module.glob(spec_pattern)
        if spec_files:
            spec_path = Path(spec_files[0])
            shutil.copy(spec_path, output_dir / "spec.md")
            metadata["spec"] = str(spec_path)

    # 6. Find plan file (if exists)
    if spec_number:
        plan_pattern = str(codev_root / "codev" / "plans" / f"{spec_number}-*.md")
        plan_files = glob_module.glob(plan_pattern)
        if plan_files:
            plan_path = Path(plan_files[0])
            shutil.copy(plan_path, output_dir / "plan.md")
            metadata["plan"] = str(plan_path)

    # Save metadata
    (output_dir / "metadata.json").write_text(json.dumps(metadata, indent=2))

    return metadata


def build_pr_query(pr_number: int, data_dir: Path, metadata: dict) -> str:
    """Build structured query for consultant."""
    files_count = len(metadata.get("files", []))
    diff_lines = metadata.get("diff_lines", 0)
    title = metadata.get("title", f"PR #{pr_number}")
    branch = metadata.get("branch", "unknown")

    query = f"""Review Pull Request #{pr_number}: {title}

Branch: {branch}
Files changed: {files_count}
Diff size: {diff_lines} lines

Available data files (read these to understand the PR):
- PR Info: {data_dir}/pr-info.txt
- Comments: {data_dir}/pr-comments.txt
- Diff: {data_dir}/pr-diff.patch
- Files JSON: {data_dir}/pr-files.json
"""

    if "spec" in metadata:
        query += f"- Specification: {data_dir}/spec.md\n"
    if "plan" in metadata:
        query += f"- Implementation Plan: {data_dir}/plan.md\n"

    query += """
Please review:
1. Code quality and correctness
2. Alignment with spec/plan (if provided)
3. Test coverage and quality
4. Edge cases and error handling
5. Documentation and comments
6. Any security concerns

End your review with a verdict in this EXACT format:

---
VERDICT: [APPROVE | REQUEST_CHANGES | COMMENT]
SUMMARY: [One-line summary of your review]
CONFIDENCE: [HIGH | MEDIUM | LOW]
---

KEY_ISSUES: [List of critical issues if any, or "None"]
"""

    return query


def extract_verdict(full_output: str) -> str:
    """Extract verdict section from consultation output.

    Returns everything from VERDICT: marker to end of output.
    Falls back to last 50 lines if no VERDICT marker found.
    """
    lines = full_output.split("\n")

    # Find VERDICT marker
    for i, line in enumerate(lines):
        if "VERDICT:" in line.upper():
            # Return from VERDICT line to end
            return "\n".join(lines[i:])

    # Fallback: return last 50 lines
    return "\n".join(lines[-50:])


def cleanup_old_pr_consultations(consult_dir: Path, keep_last: int = 10) -> int:
    """Keep only the N most recent PR consultation directories.

    Returns number of directories removed.
    """
    pr_dirs = sorted(
        [d for d in consult_dir.glob("pr-*") if d.is_dir()],
        key=lambda d: d.stat().st_mtime,
        reverse=True,
    )

    removed = 0
    for old_dir in pr_dirs[keep_last:]:
        try:
            shutil.rmtree(old_dir)
            removed += 1
        except Exception:
            pass

    return removed


def do_consult(model: str, query: Optional[str], dry_run: bool) -> None:
    """Execute a consultation with the specified model."""
    # Lazy load paths
    codev_root = find_codev_root()
    role_file = codev_root / "codev" / "roles" / "consultant.md"
    log_dir = codev_root / ".consult"

    # Load .env file
    load_dotenv(codev_root)

    # Handle stdin
    if query is None:
        if not sys.stdin.isatty():
            query = sys.stdin.read().rstrip()
            if not query:
                print("Error: Empty input from stdin", file=sys.stderr)
                sys.exit(1)
        else:
            print("Error: No query provided", file=sys.stderr)
            print("Usage: consult <model> <query>", file=sys.stderr)
            sys.exit(1)

    role = get_role(role_file)

    # Resolve model aliases
    resolved = MODEL_MAP.get(model.lower(), model.lower())
    # Track temp file for cleanup
    temp_system_file = None

    if resolved == "gemini":
        if not shutil.which("gemini"):
            print(
                "Error: gemini-cli not found.\n"
                "Install: https://github.com/google-gemini/gemini-cli",
                file=sys.stderr,
            )
            sys.exit(1)
        # gemini-cli uses GEMINI_SYSTEM_MD env var pointing to a file
        # Create a temp file with the role content
        temp_system_file = tempfile.NamedTemporaryFile(
            mode="w", suffix=".md", delete=False
        )
        temp_system_file.write(role)
        temp_system_file.close()
        cmd = ["gemini", "--yolo", query]
        env = {"GEMINI_SYSTEM_MD": temp_system_file.name}
        # Avoid "Both GOOGLE_API_KEY and GEMINI_API_KEY are set" warning
        # by unsetting GEMINI_API_KEY if GOOGLE_API_KEY is present
        if os.environ.get("GOOGLE_API_KEY") and os.environ.get("GEMINI_API_KEY"):
            env["GEMINI_API_KEY"] = ""
    elif resolved == "codex":
        if not shutil.which("codex"):
            print(
                "Error: codex not found.\n"
                "Install: npm install -g @openai/codex",
                file=sys.stderr,
            )
            sys.exit(1)
        cmd = ["codex", "exec", "--full-auto", query]
        env = {"CODEX_SYSTEM_MESSAGE": role}
    elif resolved == "claude":
        if not shutil.which("claude"):
            print(
                "Error: claude not found.\n"
                "Install: npm install -g @anthropic-ai/claude-code",
                file=sys.stderr,
            )
            sys.exit(1)
        # Claude Code: prepend role to query, use prompt mode with permission skip
        full_query = f"{role}\n\n---\n\nConsultation Request:\n{query}"
        cmd = ["claude", "--print", "-p", full_query, "--dangerously-skip-permissions"]
        env = {}
    else:
        print(
            f"Unknown model: {model}\n"
            f"Available: gemini, codex, claude (aliases: pro, gpt, opus)",
            file=sys.stderr,
        )
        sys.exit(1)

    if dry_run:
        print(f"Command: {' '.join(cmd)}")
        if env:
            for key, value in env.items():
                # Show truncated env var for long values
                if key == "GEMINI_SYSTEM_MD":
                    print(f"Env: {key}=<temp file with consultant role>")
                else:
                    preview = value[:50] + "..." if len(value) > 50 else value
                    print(f"Env: {key}={preview}")
        # Clean up temp file if created
        if temp_system_file:
            os.unlink(temp_system_file.name)
        sys.exit(0)

    # Execute with passthrough stdio, handle Ctrl+C gracefully, track timing
    full_env = {**os.environ, **(env or {})}
    start_time = time.time()
    try:
        result = subprocess.run(cmd, env=full_env)
        duration = time.time() - start_time
        log_query(log_dir, resolved, query, duration)
        print(f"\n[{resolved} completed in {duration:.1f}s]", file=sys.stderr)
        sys.exit(result.returncode)
    except KeyboardInterrupt:
        duration = time.time() - start_time
        log_query(log_dir, resolved, query, duration)
        print("\nInterrupted", file=sys.stderr)
        sys.exit(130)
    finally:
        # Clean up temp file if created
        if temp_system_file:
            try:
                os.unlink(temp_system_file.name)
            except Exception:
                pass


def do_pr(number: int, model: Optional[str], dry_run: bool) -> None:
    """Execute a PR consultation."""
    codev_root = find_codev_root()
    log_dir = codev_root / ".consult"
    pr_dir = log_dir / f"pr-{str(number).zfill(4)}"

    # Load .env
    load_dotenv(codev_root)

    # Determine which models to run
    if model:
        resolved = MODEL_MAP.get(model.lower(), model.lower())
        if resolved not in ALL_MODELS:
            print(f"Error: Unknown model '{model}'", file=sys.stderr)
            print(f"Available: {', '.join(ALL_MODELS)}", file=sys.stderr)
            sys.exit(1)
        models_to_run = [resolved]
    else:
        # Default to all models
        models_to_run = ALL_MODELS

    print(f"[PR Review #{number}]", file=sys.stderr)
    print(f"Models: {', '.join(models_to_run)}", file=sys.stderr)

    if dry_run:
        print("\n[DRY RUN] Would execute:", file=sys.stderr)
        print(f"  1. Fetch PR data to {pr_dir}/", file=sys.stderr)
        print("     - gh pr view N", file=sys.stderr)
        print("     - gh pr view N --comments", file=sys.stderr)
        print("     - gh pr diff N", file=sys.stderr)
        print("     - gh pr view N --json files,title,headRefName", file=sys.stderr)
        print("     - Copy spec/plan if found", file=sys.stderr)
        print(f"  2. Build query with file paths", file=sys.stderr)
        print(f"  3. Run consultation with: {', '.join(models_to_run)}", file=sys.stderr)
        print(f"  4. Save outputs to {pr_dir}/<model>-full.txt", file=sys.stderr)
        print(f"  5. Extract and display verdicts", file=sys.stderr)
        sys.exit(0)

    # Step 1: Fetch PR data
    print("\nFetching PR data...", file=sys.stderr)
    fetch_start = time.time()
    metadata = fetch_pr_data(number, pr_dir)
    fetch_duration = time.time() - fetch_start
    print(f"  Fetched in {fetch_duration:.1f}s", file=sys.stderr)

    files_count = len(metadata.get("files", []))
    diff_lines = metadata.get("diff_lines", 0)
    print(f"  Files: {files_count}, Diff: {diff_lines} lines", file=sys.stderr)

    if "spec" in metadata:
        print(f"  Spec: {metadata['spec']}", file=sys.stderr)
    if "plan" in metadata:
        print(f"  Plan: {metadata['plan']}", file=sys.stderr)

    # Step 2: Build query
    query = build_pr_query(number, pr_dir, metadata)

    # Step 3: Run consultations
    print(f"\nConsulting {len(models_to_run)} model(s)...", file=sys.stderr)
    consultation_start = time.time()

    results = {}

    if len(models_to_run) == 1:
        # Single model - run directly with streaming output
        model_name = models_to_run[0]
        print(f"\n{'='*60}", file=sys.stderr)
        print(f"[{model_name.upper()}] Starting consultation...", file=sys.stderr)
        print(f"{'='*60}\n", file=sys.stderr)

        output_file = pr_dir / f"{model_name}-full.txt"
        output, retcode, duration = run_model_consultation(
            model_name, query, codev_root, output_file
        )
        results[model_name] = (output, retcode, duration)

        log_query(log_dir, model_name, f"PR #{number} review", duration)

    else:
        # Multiple models - run in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_model = {
                executor.submit(
                    run_model_consultation,
                    m,
                    query,
                    codev_root,
                    pr_dir / f"{m}-full.txt",
                ): m
                for m in models_to_run
            }

            for future in concurrent.futures.as_completed(future_to_model):
                model_name = future_to_model[future]
                try:
                    output, retcode, duration = future.result()
                    results[model_name] = (output, retcode, duration)
                    log_query(log_dir, model_name, f"PR #{number} review", duration)
                    print(f"  [{model_name}] completed in {duration:.1f}s", file=sys.stderr)
                except Exception as e:
                    results[model_name] = (str(e), 1, 0.0)
                    print(f"  [{model_name}] failed: {e}", file=sys.stderr)

    consultation_duration = time.time() - consultation_start

    # Step 4: Extract and display verdicts
    print(f"\n{'='*60}")
    print("VERDICTS")
    print(f"{'='*60}")

    for model_name in models_to_run:
        if model_name in results:
            output, retcode, duration = results[model_name]
            verdict = extract_verdict(output)

            # Save verdict to file
            verdict_file = pr_dir / f"{model_name}-verdict.txt"
            verdict_file.write_text(verdict)

            print(f"\n[{model_name.upper()}] ({duration:.1f}s)")
            print("-" * 40)
            print(verdict)

    # Step 5: Summary
    total_duration = fetch_duration + consultation_duration
    print(f"\n{'='*60}")
    print("SUMMARY")
    print(f"{'='*60}")
    print(f"PR: #{number}")
    print(f"Pre-fetch: {fetch_duration:.1f}s")
    print(f"Consultation: {consultation_duration:.1f}s")
    print(f"Total: {total_duration:.1f}s")
    print(f"Output: {pr_dir}/")

    # Step 6: Cleanup old consultations
    removed = cleanup_old_pr_consultations(log_dir, keep_last=10)
    if removed > 0:
        print(f"Cleaned up {removed} old PR consultation(s)", file=sys.stderr)


def main():
    """Main entry point with manual argument parsing for hybrid CLI."""
    # Check for --help at any position for the main command
    if len(sys.argv) == 1 or (len(sys.argv) == 2 and sys.argv[1] in ["--help", "-h"]):
        print("""Usage: consult <model> <query> [OPTIONS]
       consult pr <number> [OPTIONS]

Consult external AI models for second opinions.

Commands:
  consult <model> <query>   Consult a model with a query
  consult pr <number>       Review a Pull Request with optimized fetching

Arguments (default command):
  model                     Model: gemini, codex, claude (or aliases: pro, gpt, opus)
  query                     Query (or pipe via stdin)

Options (default command):
  -n, --dry-run            Print command without executing
  -h, --help               Show this message and exit

PR Subcommand Options:
  -m, --model MODEL        Specific model (default: all three in parallel)
  -n, --dry-run            Show what would be fetched without executing

Examples:
  consult gemini "Review this architecture"
  consult pro "Review this design"
  echo "Review this" | consult gpt
  consult pr 33
  consult pr 33 --model gemini
  consult pr 33 --dry-run
""")
        sys.exit(0)

    # Check if first arg is "pr" subcommand
    if len(sys.argv) >= 2 and sys.argv[1].lower() == "pr":
        # PR subcommand
        args = sys.argv[2:]

        # Check for help
        if "--help" in args or "-h" in args:
            print("""Usage: consult pr <number> [OPTIONS]

Consult on a Pull Request with optimized data fetching.

Pre-fetches PR data (info, comments, diff, files) and passes file paths
to the consultant model, reducing git command overhead.

Arguments:
  number                   PR number to review [required]

Options:
  -m, --model MODEL        Specific model (gemini, codex, claude)
  -n, --dry-run            Show what would be fetched without executing
  -h, --help               Show this message and exit

Examples:
  consult pr 33                  # All models in parallel (default)
  consult pr 33 --model gemini   # Single model
  consult pr 33 --dry-run        # Show what would be fetched
""")
            sys.exit(0)

        # Parse PR number
        if not args:
            print("Error: PR number required", file=sys.stderr)
            print("Usage: consult pr <number>", file=sys.stderr)
            sys.exit(1)

        try:
            pr_number = int(args[0])
        except ValueError:
            print(f"Error: Invalid PR number '{args[0]}'", file=sys.stderr)
            sys.exit(1)

        # Parse options
        model = None
        dry_run = False

        i = 1
        while i < len(args):
            arg = args[i]
            if arg in ["--model", "-m"]:
                if i + 1 >= len(args):
                    print("Error: --model requires a value", file=sys.stderr)
                    sys.exit(1)
                model = args[i + 1]
                i += 2
            elif arg in ["--dry-run", "-n"]:
                dry_run = True
                i += 1
            else:
                print(f"Error: Unknown option '{arg}'", file=sys.stderr)
                sys.exit(1)

        do_pr(pr_number, model, dry_run)

    else:
        # Default consult command
        args = sys.argv[1:]

        # Parse options
        dry_run = False
        positional_args = []

        i = 0
        while i < len(args):
            arg = args[i]
            if arg in ["--dry-run", "-n"]:
                dry_run = True
                i += 1
            elif arg.startswith("-"):
                print(f"Error: Unknown option '{arg}'", file=sys.stderr)
                sys.exit(1)
            else:
                positional_args.append(arg)
                i += 1

        if len(positional_args) < 1:
            print("Error: Model is required", file=sys.stderr)
            print("Usage: consult <model> <query>", file=sys.stderr)
            sys.exit(1)

        model = positional_args[0]
        query = positional_args[1] if len(positional_args) > 1 else None

        do_consult(model, query, dry_run)


if __name__ == "__main__":
    main()
